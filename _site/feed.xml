<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko-KR"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="ko-KR" /><updated>2024-03-23T11:36:38+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eunsoo Kang</title><subtitle>Joy of Growth</subtitle><author><name>Eunsoo Kang</name><email>me@eunsour.dev</email></author><entry><title type="html">[리뷰] Training Compute-Optimal Large Language Models</title><link href="http://localhost:4000/chinchilla-scaling-laws/" rel="alternate" type="text/html" title="[리뷰] Training Compute-Optimal Large Language Models" /><published>2024-01-29T11:58:47+09:00</published><updated>2024-01-29T11:58:47+09:00</updated><id>http://localhost:4000/chinchilla-scaling-laws</id><content type="html" xml:base="http://localhost:4000/chinchilla-scaling-laws/"><![CDATA[<p><a href="https://arxiv.org/abs/2203.15556">본 논문</a>은 2022년 3월에 발표되었다. 본 논문의 주요 목표는 주어진 컴퓨팅 예산에서 언어 모델을 훈련하기 위한 최적의 모델 크기와 토큰 수를 조사하는 것이다.</p>

<p>그동안 <a href="https://arxiv.org/abs/2001.08361">Scaling Laws</a>에 따라 모델의 매개변수 수와 성능 사이에 법칙 관계가 있음을 발견하고 점점 더 큰 규모의 모델을 훈련해왔다. 하지만 저자는 175B GPT-3, 280B Gopher 및 530B Megatron과 같은 현재의 LLM이 underfitting 되었다고 언급하며, 학습 토큰 수와 모델 크기를 동일하게 조정해야 한다고 주장한다.</p>

<p>70M에서 16B 이상의 파라미터를 가진 400개 이상의 언어 모델을 5B에서 500B의 토큰으로 훈련한 결과, 컴퓨팅 최적화 훈련을 위해서는 모델 크기와 학습 토큰 수를 동일하게 확장해야 한다는 것을 발견했다. 즉, 모델 크기가 두 배가 될 때마다 학습 토큰 수도 두 배가 되어야 한다. 이를 검증하기 위해, 280B Gopher와 동일한 컴퓨팅 예산을 사용하면서도 70B 매개변수와 4배 더 많은 훈련 데이터를 포함하는 <strong>Chinchilla</strong>라는 새로운 LLM을 훈련하였다.</p>

<center><img src="/assets/img/chinchilla/0.png" style="zoom: 65%;" /></center>

<p><strong>Chinchilla</strong>는 다양한 다운스트림 태스크 평가에서 Gopher(280B), GPT-3(175B), Jurassic-1(178B), 및 Megatron(530B)보다 일관되고 유의미한 성능을 보였다.</p>

<p>이 연구의 주요 질문은 <strong>“고정된 FLOPs 예산이 주어졌을 때, 모델 크기와 학습 토큰 수를 어떻게 조정해야 할까?”</strong>이다. 이를 위해 세 가지 접근 방식을 시도했는데, 먼저 컴퓨팅과 모델 크기 사이의 거듭제곱 관계를 가정하였다.</p>

<p><br /></p>

<h2 id="estimating-the-optimal-parametertraining-tokens-allocation">Estimating the optimal parameter/training tokens allocation</h2>
<h4 id="approach-1-fix-model-sizes-and-vary-number-of-training-tokens">Approach 1: Fix model sizes and vary number of training tokens</h4>

<center><img src="/assets/img/chinchilla/1.png" style="zoom: 65%;" /></center>

<p>첫 번째 접근 방식에서는 고정된 모델 크기(75M, 250M, 500M, 1B, 2.5B, 5B, 10B)에 대해 고정된 FLOPs로 학습 토큰 수를 변경하였다. 거듭제곱 법칙(\(N_{opt} \propto C^{a}\) 및 \(D_{opt} \propto C^b\))을 사용하여 Gopher의 컴퓨팅 예산(\(5.76 \times 10^{23}\))에 대한 최적의 모델 크기가 67B이며, 이에 대응하는 학습 토큰 수는 1.5T라는 것을 알아냈다.</p>

<p><br /></p>

<h4 id="approach-2-isoflop-profiles">Approach 2: IsoFLOP profiles</h4>

<center><img src="/assets/img/chinchilla/2.png" style="zoom: 65%;" /></center>

<p>두 번째 접근 방식에서는 9개의 서로 다른 학습 FLOP (\(6 \times 10^{18}\) 에서  \(3 \times 10^{21}\) FLOPs 범위)으로 구성된 고정된 세트에 대해 모델 크기를 변경하고 각 지점에 대한 최종 학습 손실을 고려하였다. 이를 통해 <strong>“주어진 FLOP 예산에 대해 최적의 파라미터 수는 얼마인가?”</strong>에 대한 질문에 답하고자 하였다.</p>

<p>각 IsoFLOPs 곡선에 포물선을 맞춰 최소 손실이 달성되는 모델 크기를 직접 추정하였다. 그런 다음, 이전 접근 방식과 마찬가지로 FLOP과 최적의 모델 크기 및 학습 토큰 수 사이에 거듭제곱 법칙(\(N_{opt} \propto C^{a}\) 및 \(D_{opt} \propto C^b\))을 적용하였다.</p>

<p>이 접근 방식은 Gopher의 컴퓨팅 예산에 대한 최적의 모델 크기가 63B이고 학습토큰이 1.4T가 되어야 함을 시사한다.</p>

<p><br /></p>

<h4 id="approach-3-fitting-a-parametric-loss-function">Approach 3: Fitting a parametric loss function</h4>

<center><img src="/assets/img/chinchilla/3.png" style="zoom: 65%;" /></center>

<p>세 번째 접근 방식에서는 위의 두 가지 접근 방식의 결과를 결합하여 모델 매개변수와 토큰 수의 매개변수 함수로 표현하고자 하였다. 이를 위해 함수형을 제안하고 Huber 손실을 최소화하여 Gopher Flop 예산에 대한 최적의 모델 크기를 40B 매개변수로 추정하였다.</p>

<p><br /></p>

<h2 id="chinchilla">Chinchilla</h2>

<p>섹션 3에 따르면 Gopher의 컴퓨팅 예산에 대한 최적의 모델 크기는 40B에서 70B 사이이다. 이를 검증하기 위해 1.4T 토큰의 데이터셋으로 70B 크기의 모델을 훈련하고 <strong>Chinchilla</strong>라고 명명하였다. 이후 이 모델을 Gopher 및 다른 LLM과 비교하여 성능을 평가하였다.</p>

<p>Chinchilla를 학습시키는 데 사용되는 전체 하이퍼파라미터는 다음 표와 같다.</p>

<center><img src="/assets/img/chinchilla/4.png" style="zoom: 65%;" /></center>

<p><br /></p>

<h3 id="results">Results</h3>

<center><img src="/assets/img/chinchilla/5.png" style="zoom: 65%;" /></center>

<p>Chinchilla는 위의 Language Modeling 및 downstream task에서 평가하였다.</p>

<p><br /></p>

<h4 id="language-modeling">Language Modeling</h4>

<center><img src="/assets/img/chinchilla/6.png" style="zoom: 65%;" /></center>

<p>그림 5는 The Pile이라는 모든 평가 데이터셋에 대한 결과를 Gopher와 비교한 결과이다. Chinchilla는 Gopher를 크게 능가하는 성능을 보여주고 있으며, Jurassic-1 (178B)과 비교했을 때도 두 개의 태스크(<u>dm_mathematics</u> 및 <u>ubuntu_irc</u>)를 제외한 모든 subset에서 Chinchilla가 우세한 것을 확인할 수 있다.</p>

<p><br /></p>

<h4 id="mmlu">MMLU</h4>

<center><img src="/assets/img/chinchilla/7.png" style="zoom: 65%;" /></center>

<p>MMLU는 모델의 멀티태스크 정확도를 측정하는 데이터셋으로, 인문학, 초등 수학, 역사, 컴퓨터 과학, 법률 등 57개의 주제를 포함하고 있다. 이 벤치마크에서 Chinchilla는 평균 정확도가 67.6%로, 훨씬 작은 크기에도 불구하고 Gopher를 크게 앞섰다. Gopher보다 7.6% 향상된 성능을 보였으며, 2023년 6월 전문가들이 예측한 정확도 63.4%를 능가하는 정확도를 기록하였다.</p>

<p><br /></p>

<h2 id="conclusion">Conclusion</h2>

<p>최근 대규모 언어 모델 학습의 추세는 학습 토큰 수를 늘리지 않고 모델 크기를 늘리는 것이었다. 그러나 400회 이상의 훈련 결과를 바탕으로 세 가지 접근 방식을 제안하였고, 이를 통해 Gopher의 크기가 상당히 과대하다는 것을 확인하였다. 더 많은 데이터로 훈련된 더 작은 모델인 Chinchilla가 Gopher는 물론 더 큰 규모의 모델보다 성능이 뛰어난 것을 확인할 수 있었다.</p>

<p>따라서 모델 크기를 계속해서 늘리는 것보다 데이터셋의 확장에 더 집중해야 하며, 데이터셋의 품질에 중점을 두고 책임감 있게 수집해야 한다. 또한 대규모 모델을 훈련하는 데는 많은 비용이 들기 때문에, 최적의 모델 크기를 계산하고 학습 단계를 미리 선택하는 것이 중요한데, <strong>Chinchilla Scaling Law</strong>는 대규모 언어 모델의 성능과 기능 향상을 위한 컴퓨팅 최적화 접근 방식을 제공한다.</p>

<p><br /></p>]]></content><author><name>Eunsoo Kang</name><email>me@eunsour.dev</email></author><category term="NLP" /><category term="LLM" /><summary type="html"><![CDATA[본 논문은 2022년 3월에 발표되었다. 본 논문의 주요 목표는 주어진 컴퓨팅 예산에서 언어 모델을 훈련하기 위한 최적의 모델 크기와 토큰 수를 조사하는 것이다.]]></summary></entry><entry><title type="html">[리뷰] LLaMA: Open and Efficient Foundation Language Models</title><link href="http://localhost:4000/LLaMA-1/" rel="alternate" type="text/html" title="[리뷰] LLaMA: Open and Efficient Foundation Language Models" /><published>2023-12-24T11:58:47+09:00</published><updated>2023-12-24T11:58:47+09:00</updated><id>http://localhost:4000/LLaMA-1</id><content type="html" xml:base="http://localhost:4000/LLaMA-1/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>최근 몇 년간, 언어 모델의 크기는 <a href="https://arxiv.org/abs/2001.08361">Scaling laws</a>에 따라 지속적으로 증가하는 추세였다. 이는 모델의 성능 향상에는 기여했지만, 모델을 서빙할 때의 추론 예산을 무시하는 문제점이 있었다. 따라서 기업들은 목표 성능을 달성한 후에는 학습 속도가 아닌 <u>추론 속도가 가장 빠른 모델</u>을 선호하는 경향이 있다.</p>

<p>2022년 5월 Meta에서 <a href="https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/">OPT-175B</a>를 발표하였다. 하지만 OPT는 <a href="https://github.com/openai/gpt-3">GPT-3</a>와 동일한 모델 크기를 갖고 있음에도 불구하고 성능에서 큰 차이가 있었다. 논문에서도 모델의 성능을 자랑하는 것보다는 왜 이 모델이 성능이 부족했는지에 대한 탐구에 중점을 두고 있다.</p>

<p>이후 Meta는 절치부심해서 새로운 모델을 개발하였으며, Google Deepmind에서 발표한 <a href="https://arxiv.org/abs/2203.15556">Chinchilla Scaling Laws</a> 연구 결과에 영감을 받아 GPT-3(175B)보다 더 작으면서 고성능인 <strong>LLaMA</strong>을 선보였다.</p>

<p>GPT-3가 1.3B부터 175B까지 네 가지 모델을 제공한 것과 마찬가지로, LLaMA도 7B, 13B, 33B, 65B의 네 가지 모델을 제공한다.</p>

<p><br /></p>

<h2 id="approach">Approach</h2>
<h3 id="pre-training-data">Pre-training Data</h3>

<p><strong>LLaMA</strong>는 <u>공개적으로 사용 가능</u>하고 <u>오픈 소스와 호환되는 데이터</u>만 사용한다는 제한을 두고 다른 LLM을 학습하는 데 활용된 데이터 셋을 재사용하였다. 사용된 데이터셋과 데이터의 비율은 아래와 같다.</p>

<center><img src="/assets/img/llama_1/0.png" style="zoom: 65%;" /></center>

<p>데이터셋 구성을 보면 <strong>CommonCrawl</strong>, <strong>C4</strong>의 합계가 82%로, 이는 GPT-3에 투입된 비율과 같다.</p>

<p>웹 크롤링 데이터는 품질이 낮기 때문에 정제를 잘하는 것이 중요하다. 이를 위해 Meta에서는 별도의 모델을 만들어 수집한 페이지가 Wikipedia의 인용 출처로 쓰일 수 있는지 여부를 판별하는 <u>분류기를 학습</u>시키고, 이를 통과한 페이지만 학습에 투입하였다.</p>

<p>다음으로 Google BigQuery에서 사용되는 공개 <strong>GitHub</strong> 데이터셋을 사용하였다. 코드 데이터의 논리성이 언어 학습에 도움을 준다고 한다.</p>

<p>이외에 2022년 6월부터 8월까지 20개국의 <strong>Wikipedia</strong> 데이터가 높은 비중으로 학습에 투입되었고, 논리적인 태스크 수행을 위해 <strong>도서</strong>와 <strong>논문</strong> 데이터도 포함된 것으로 보인다.</p>

<p>토크나이저는 OPT에서 사용했던 GPT 스타일의 <a href="https://arxiv.org/abs/1508.07909">BBPE</a>에서 <a href="https://github.com/google/sentencepiece">BPE</a>로 변경하였고, Unknown 토큰을 대비하여 폴백 옵션을 추가하였다.</p>

<p><br /></p>

<h3 id="architecture">Architecture</h3>

<p>LLaMA의 아키텍처는 <u>Vanilla Transformer 아키텍처</u>에서 빠른 학습을 위해 일부 구조를 개선하였다.</p>

<ul>
  <li><strong>Pre-normalization</strong> [from GPT-3]
    <ul>
      <li>Vanilla Transformer와 달리 attention 전에 normalization을 먼저 수행하는 Pre-normalization을 적용하였으며, <a href="https://github.com/google-research/t5x">T5</a>와 마찬가지로 <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>을 사용하였다.</li>
    </ul>
  </li>
  <li><strong>SwiGLU activation function</strong> [from <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM</a>]
    <ul>
      <li>성능 개선을 위해 ReLU를 SwiGLU로 교체하였으며, PaLM과 달리 \(4d\) 가 아닌 \(\frac{2}{3}4d\) 를 사용하였다.</li>
      <li><a href="https://arxiv.org/abs/1710.05941v1">Swish</a>와 <a href="https://arxiv.org/abs/1612.08083">GLU</a>를 합친 것인데 ReLU보다 부드러운 곡선을 그리며, <a href="https://arxiv.org/abs/1606.08415">GELU</a>와 비슷하지만 연산이 간단하다는 장점이 있다.</li>
    </ul>
  </li>
  <li><strong>Rotary Embeddings</strong> [from <a href="https://github.com/EleutherAI/gpt-neo">GPTNeo</a>]
    <ul>
      <li>absolute positional embeddings를 제거하는 대신 <a href="https://arxiv.org/abs/2104.09864">RoPE (rotary positional embeddings)</a> 방식을 사용하여 학습 시 2048 토큰보다 긴 입력도 처리할 수 있다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="optimizer">Optimizer</h3>

<p>LLaMA는 다음과 같은 하이퍼파라미터를 사용하여 <a href="https://arxiv.org/pdf/1711.05101.pdf">AdamW Optimizer</a>를 통해 학습된다.</p>

<ul>
  <li>\(\beta_1\) = 0.9, \(\beta_2\) = 0.95</li>
  <li>weight decay = 0.1</li>
  <li>gradient clipping = 1.0</li>
  <li>warmup steps = 2000</li>
  <li>최종 학습률이 최대 학습률의 10%가 되도록 cosine learning rate scheduler 사용</li>
</ul>

<p><br /></p>

<center><img src="/assets/img/llama_1/1.png" /></center>

<p><br /></p>

<h3 id="efficient-implementation">Efficient implementation</h3>

<p>모델의 학습 속도 향상을 위해 몇 가지 최적화를 수행하였다.</p>

<ol>
  <li>메모리 사용량과 런타임을 줄이기 위해 multi-head attention을 효율적으로 구현한 <code class="language-plaintext highlighter-rouge">xformers</code> 라이브러리를 사용하였다.</li>
  <li>backward pass시 linear layer의 출력과 같이 계산 비용이 많이 드는 activation을 그대로 저장함으로써 재계산되는 activation을 줄였다.</li>
  <li>activation의 계산과 네트워크를 통한 GPU 간의 통신을 최대한 중첩시켜 수행하였다.</li>
</ol>

<p><br /></p>

<p>이로써 학습 시간은 LLaMA-65B 학습을 80G A100 2048장, 256노드로 진행해서 1.4T 토큰 학습에 21일 걸렸다고 한다.</p>

<center><img src="/assets/img/llama_1/2.png" style="zoom: 65%;" /></center>

<p><br /></p>

<h2 id="main-results">Main results</h2>

<p>저자들은 Zero-shot 및 Few-shot task를 포함한 다양한 벤치마크에서 GPT-3, Gopher, Chinchilla, PaLM과 같은 Foundation Model과 LLaMA를 비교하였다. 이중 대표적인 벤치마크에 대한 결과는 다음과 같다.</p>

<center><img src="/assets/img/llama_1/3.png" /></center>

<ul>
  <li>LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B보다 성능이 뛰어나다.</li>
  <li>LLaMA-65B는 BoolQ와 WinoGrande를 제외한 모든 벤치마크에서 PaLM540B보다 성능이 뛰어나다.</li>
  <li>LLaMA-13B 또한 GPT-3에 비해 10배 더 작은 크기임에도 불구하고 PIQA, OBQA를 제외한 대부분의 벤치마크에서 우수한 성능을 보인다.</li>
</ul>

<p>해당 결과를 통해, 같은 모델 크기에서 LLaMA가 경쟁 모델 대비 우수한 성능을 보이며, 작은 모델에서도 일정 수준 이상의 성능을 발휘할 수 있음을 확인할 수 있다.</p>

<p><br /></p>

<h3 id="massive-multitask-language-understanding5">Massive Multitask Language Understanding5</h3>

<center><img src="/assets/img/llama_1/4.png" /></center>

<ul>
  <li><strong>MMLU</strong>는 모델의 멀티태스크 정확도를 측정하는 데이터셋으로 인문학, 초등 수학, 역사, 컴퓨터 과학, 법률 등 57개의 주제를 포함하고 있다. MMLU를 잘 수행하기 위해서는 모델이 광범위한 상식을 갖추고 다양한 문제를 해결할 수 있는 능력이 필요하다.</li>
  <li>LLaMA-65B는 GPT-3보다 더 우수하지만 Chinchilla와 PaLM에 비해 떨어지는 벤치마크 결과를 보인다.</li>
  <li>저자들은 책과 논문 데이터가 Gopher와 Chinchilla에 비해 적은 양으로 학습되었기 때문이라고 분석하고 있다. (LLaMA : 177GB, Gopher, Chinchilla : 2TB)</li>
</ul>

<p><br /></p>

<h2 id="instruction-tuning">Instruction Tuning</h2>

<p>다음은 LLaMA-65B에 instruction tuning을 적용하고 MMLU에서 5-shot 평균을 낸 결과이다.</p>

<p>아주 적은 양의 명령어 데이터 파인 튜닝만으로 MMLU의 성능이 향상되고 instruction 수행 능력이 더욱 향상되는 것을 관찰할 수 있었다. (63.4 → 68.9)</p>

<p>하지만 77.4%라는 SOTA 성능을 끌어낸 OpenAI의 <a href="https://platform.openai.com/docs/models/gpt-3-5">code-davinci-002</a> 모델에는 미치지 못하는 모습을 볼 수 있다.</p>

<center><img src="/assets/img/llama_1/5.png" style="zoom: 65%;" /></center>

<p><br /></p>

<h2 id="couclusion">Couclusion</h2>

<p><strong>LLaMA-13B</strong>는 GPT-3의 10분의 1 크기에도 불구하고, 대부분의 벤치마크에서 GPT-3보다 높은 성능을 보였다. 한 발 더 나아가, LLaMA-65B는 Chinchilla, Gopher, GPT-3, PaLM 등의 경쟁 모델들과 비슷하거나 더 나은 결과를 보였다.</p>

<p>OPT의 성능 저하 원인 중 하나는 180B 토큰으로 학습되어 학습량이 부족하다는 점이었다. (GPT-3의 경우 300B 토큰으로 학습) 이에 비해 1.4T 토큰으로 학습한 LLaMA는 더 많은 학습 데이터를 기반으로 뛰어난 성능을 보이는 것은 이전에 Chinchilla에서 확인된 바와 같이 예상 가능한 결과이다.</p>

<p>더불어, Chinchilla보다 LLaMA의 성능이 대체로 높은 것으로 나타났는데, 이는 정제된 웹 크롤링 데이터를 활용한 것이 효과를 발휘한 것으로 해석할 수 있다.</p>

<p>이후 LLaMA 연구진들은 향후 더 큰 모델에 더 많은 token을 학습시킨 모델을 발표할 계획이라고 밝혔다.</p>

<p>그리고 2023년 7월, Meta AI팀은 학술적 및 상업적 이용이 전부 가능한 오픈소스 LLM으로 <a href="https://arxiv.org/abs/2307.09288">LLaMA-2</a>를 발표하였다. 이 모델은 LLaMA-1보다 40% 더 많은 데이터(2T)를 활용하고, context length도 2배로 늘어난(2048에서 4096으로) 특징을 가지고 있다.</p>

<p><br /></p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a></li>
  <li><a href="https://devocean.sk.com/blog/techBoardDetail.do?ID=164601">https://devocean.sk.com/blog/techBoardDetail.do?ID=164601</a></li>
</ul>

<p><br /></p>]]></content><author><name>Eunsoo Kang</name><email>me@eunsour.dev</email></author><category term="NLP" /><category term="LLM" /><category term="Generative AI" /><category term="Foundation Models" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Google “We Have No Moat, And Neither Does OpenAI”</title><link href="http://localhost:4000/google-we-have-no-moat-and-neither/" rel="alternate" type="text/html" title="Google “We Have No Moat, And Neither Does OpenAI”" /><published>2023-12-02T12:30:47+09:00</published><updated>2023-12-02T12:30:47+09:00</updated><id>http://localhost:4000/google-we-have-no-moat-and-neither</id><content type="html" xml:base="http://localhost:4000/google-we-have-no-moat-and-neither/"><![CDATA[<blockquote>
  <p>아래 내용들은 Google 내부에서 유출된 문서로 그 진위 여부를 확인한 내용이며, 회사 전체가 아닌 Google 직원의 의견일 뿐이라고 말하고 있다.</p>
</blockquote>

<p><br /></p>

<h2 id="we-have-no-moat-and-neither-does-openai">We Have No Moat And neither does OpenAI</h2>

<p><strong>“우리에게는 해자가 없으며, 이는 OpenAI도 마찬가지이다.”</strong><br /><br /></p>

<center><img src="/assets/img/we_have_no_moat/1.jpeg" /></center>
<p><br /></p>

<p>여기서 말하는 <strong>해자</strong>란 어떤 기업이 가지고 있는 고유의 특허, 기술 등을 지칭하며 다른 기업이 쉽게 넘볼 수 없는, 진입장벽이 높은 유무형의 자산을 일컫는다.<br /><br /></p>

<p>Google과 OpenAI가 arms race, 즉 군비 경쟁을 벌이는 동안 제 3의 세력인 <strong><u>오픈 소스 진영</u></strong>이 조용히 <strong><u>Major Open Problems</u></strong>라고 여겨지는 문제들을 해결하여 사람들에게 제공하고 있었다. 그 중 몇 가지 예는 다음과 같다.</p>

<ul>
  <li><strong>LLMs on a Phone</strong> : 사람들은 초당 5토큰으로 Pixel 6에서 <u>Foundation Model</u>을 실행하고 있다.</li>
  <li><strong>Scalable Personal AI</strong> : 저녁에 노트북으로 개인화된 AI를 fine-tuning할 수 있다.</li>
  <li><strong>Responsible Release</strong> : 웹 사이트 전체가 아무런 제한 없이 Art model로 가득 차 있으며 텍스트도 크게 뒤처지지 않는다.</li>
  <li><strong>Multimodality</strong> : 현재 멀티모달 SienceQA SOTA는 한 시간 만에 학습되었다.</li>
</ul>

<p><br />
여전히 품질은 Google이 앞서고 있지만 격차는 점점 빠르게 좁혀지고 있다. <br /><br /></p>

<p>제 3의 세력들은 우리가 1,000만 달러와 540B의 파라미터로 어려움을 겪던 일을 1억 달러와 13B의 매개변수로 몇 달이 아니라 몇 주 만에 해결하고 있다. 이는 다음과 같은 시사점을 가진다.</p>

<ul>
  <li><strong>Google에는 비법이 없다.</strong> Google의 최선의 희망은 다른 사람들이 Google 외부에서 하고 있는 일에서 배우고 협력하는 것이다. 3P 통합을 활성화하는 데 우선순위를 두어야 한다.</li>
  <li><strong>사용 제한이 없는 고품질의 무료 대안이 있을 때 사람들은 제한된 모델에 비용을 지불하지 않을 것이다.</strong></li>
  <li><strong>Large model은 우리의 속도를 늦추고 있다.</strong> 장기적으로 볼 때 가장 좋은 모델은 빠르게 반복할 수 있는 모델이다. 이제 20B 미만의 매개변수 체제에서 무엇이 가능한지 알았으니 작은 모델을 더 이상 사후 고려사항으로 두지 말아야 한다.</li>
</ul>

<p><br /></p>

<center><img src="/assets/img/we_have_no_moat/chart.png" /></center>

<p><br /></p>

<h2 id="what-happend">What Happend</h2>

<p>2023년 3월, Meta의 <strong>LLaMA</strong>가 <a href="https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama">대중에게 유출</a>되면서 오픈 소스 커뮤니티는 처음으로 제대로 된 기능을 갖춘 <u>foundation model</u>을 손에 넣었다. 이 모델에는 instruction이나 conversation tuning도 없었고, RLHF도 없었다. 그럼에도 불구하고 커뮤니티는 자신들이 받은 foundation model의 중요성을 즉시 이해했다.</p>

<p>그 후 엄청난 혁신이 쏟아져 나왔고, 주요 개발이 단 며칠 사이에 이루어졌다. 한 달도 채 지나지 않아, <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Instruction tuning</a>, <a href="https://github.com/ggerganov/llama.cpp">Quantization</a>, <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Quality improvements</a>, <a href="https://arxiv.org/pdf/2303.16199.pdf">Human evals</a>, <a href="https://arxiv.org/pdf/2303.16199.pdf">Multimodality</a>, <a href="https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view">RLHF</a> 등 다양한 변형이 등장했으며, 이 중 상당수는 서로를 기반으로 발전하였다.</p>

<p>가장 중요한 것은 누구나 손댈 수 있을 정도로 스케일링 문제를 해결했다는 점이다. 새로운 아이디어의 대부분은 평범한 사람들에게서 나왔으며, 학습과 실험에 대한 진입 장벽이 대형 연구 기관의 총체적인 결과물에서 <u>한 사람, 저녁 시간, 구형 노트북</u>으로 낮아졌다.</p>

<p><br /></p>

<h2 id="why-we-could-have-seen-it-coming">Why We Could Have Seen It Coming</h2>

<p>이것은 여러 면에서 놀라운 일이 아니다. 현재 오픈 소스 LLM의 르네상스는 이미지 생성의 르네상스에 이어 뜨겁게 달아오르고 있으며, 커뮤니티에서도 많은 사람들이 이 시기를 LLM의 “<strong><a href="https://simonwillison.net/2023/Mar/11/llama/">Stable Diffusion Moment</a></strong>“이라고 부른다.</p>

<p>두 경우 모두 <strong><a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation</a>(LoRA)</strong>이라는 훨씬 저렴한 Fine-Tuning 메커니즘과 스케일 면에서 획기적인 발전(이미지 합성의 경우 <a href="https://arxiv.org/abs/2112.10752">latent diffusion</a>, LLM의 경우 <a href="https://arxiv.org/abs/2203.15556">Chinchilla</a>)이 결합되어 저비용으로 대중의 참여가 가능해졌다. 두 경우 모두, 충분히 높은 품질의 모델에 대한 액세스는 전 세계 개인과 기관의 아이디어와 반복을 촉발시켰으며, 모두 대형 업체들을 빠르게 앞질렀다.</p>

<p>이러한 기여는 이미지 생성 분야에서 중추적인 역할을 했으며, <strong>Stable Diffusion</strong>은 Dall-E와는 다른 길을 걷게 되었다. 개방형 모델을 채택함으로써 제품 통합, 마켓플레이스, 사용자 인터페이스, 그리고 Dall-E에는 없던 혁신이 이루어졌다.</p>

<p>그리고 그 효과는 분명했다. 문화적 영향력 측면에서 빠르게 우위를 점한 OpenAI의 Dall-E는 점점 더 무의미해졌다. LLM에서도 같은 일이 일어날지는 아직 미지수이지만 큰 구조적 요소는 동일하다.</p>

<p><br /></p>

<h2 id="what-we-missed">What We Missed</h2>

<p>최근 오픈소스의 성공을 이끈 혁신은 여전히 우리가 고민하고 있는 문제를 직접적으로 해결해 준다. 이러한 작업에 더 많은 관심을 기울인다면 같은 일을 반복하지 않을 수 있다.</p>

<h3 id="lora는-우리가-더-주목해야-할-믿을-수-없을-정도로-강력한-기술이다">LoRA는 우리가 더 주목해야 할 믿을 수 없을 정도로 강력한 기술이다.</h3>

<p><strong><a href="https://arxiv.org/abs/2106.09685">LoRA</a></strong>는 모델 업데이트를 <u>low-rank factorizations</u>로 표현하여 업데이트 행렬의 크기를 최대 수천 배까지 줄이는 방식으로 작동한다. 따라서 적은 비용과 시간으로 모델을 Fine-Tuning할 수 있다. 소비자 하드웨어에서 몇 시간 만에 언어 모델을 개인화할 수 있다는 것은 특히 새롭고 다양한 지식을 거의 실시간으로 통합해야 하는 경우 큰 의미가 있다. 그러나 이 기술이 Google의 가장 야심찬 프로젝트 중 일부에 직접적인 영향을 미치고 있음에도 불구하고 Google 내부에서 제대로 활용되지 않고 있다.</p>

<p><br /></p>

<h2 id="retraining-models-from-scratch-is-the-hard-path">Retraining models from scratch is the hard path</h2>

<p><strong>모델을 처음부터 재학습하는 것은 어려운 방법이다.</strong></p>

<p><strong>LoRA</strong>가 효과적인 이유 중 하나는 다른 형태의 Fine-Tuning과 마찬가지로 누적 학습이 가능하다는 점이다. instruction tuning과 같은 개선 사항을 적용한 다음 다른 기여자가 대화, 추론 또는 도구 사용을 추가할 때 이를 활용할 수 있다. 개별적인 Fine-Tuning은 low-rank이지만, 그 총합은 그럴 필요가 없으므로 시간이 지남에 따라 모델에 대한 full-rank 업데이트가 누적될 수 있다.</p>

<p>즉, 새롭고 더 나은 데이터 셋과 태스크를 사용할 수 있게 되면 <u>전체 실행 비용을 지불하지 않고도 모델을 저렴하게 최신 상태로 유지</u>할 수 있다.</p>

<p>반면, 거<u>대한 모델을 처음부터 학습시키면 사전 학습뿐만 아니라 그 위에 이루어진 반복적인 개선 사항도 모두 버려지게 된다.</u> 오픈 소스 세계에서는 이러한 개선 사항이 지배적이기까지 오래 걸리지 않으므로 전체 재학습에 엄청난 비용이 소요된다.</p>

<p>우리는 각각의 새로운 애플리케이션이나 아이디어에 대해 정말로 완전히 새로운 모델이 필요한지 신중하게 생각해야 한다. 모델 가중치를 직접 재사용할 수 없을 정도로 아키텍처가 크게 개선되었다면 이전 세대의 기능을 최대한 유지할 수 있는 보다 공격적인 형태의 <u>distillation</u>에 투자해야 한다.</p>

<p><br /></p>

<h2 id="large-models-arent-more-capable-in-the-long-run-if-we-can-iterate-faster-on-small-models">Large models aren’t more capable in the long run if we can iterate faster on small models</h2>

<p><strong>작은 모델을 더 빠르게 반복할 수 있다면 큰 모델은 장기적으로 더 나은 성능을 발휘할 수 없다.</strong></p>

<p><strong>LoRA 업데이트는</strong> 가장 인기 있는 모델 사이즈의 경우 제작 비용이 매우 저렴하다(~$100). 즉, 아이디어만 있으면 거의 모든 사람이 LoRA를 이용하여 모델을 제작하고 배포할 수 있으며, 학습 시간은 하루가 채 걸리지 않는 것이 일반적이다. 이 정도 속도라면 이러한 모든 Fine-Tuning의 누적 효과가 모델의 크기로 인한 단점을 극복하는 데 그리 오랜 시간이 걸리지 않는다. 실제로 엔지니어 시간 측면에서 볼 때, 이러한 모델의 개선 속도는 가장 큰 모델로 할 수 있는 것보다 훨씬 빠르며, <a href="https://bair.berkeley.edu/blog/2023/04/03/koala/">최고의 모델은 이미 ChatGPT와 거의 구별할 수 없을 정도이다.</a> 지구상에서 가장 큰 모델을 유지 관리하는 데 집중하면 오히려 불리한 상황에 처하게 된다.</p>

<p><br /></p>

<h2 id="data-quality-scales-better-than-data-size">Data quality scales better than data size</h2>

<p><strong>데이터 품질은 데이터 크기보다 더 잘 확장된다.</strong></p>

<p>프로젝트 중 상당수는 고도로 선별된 소규모 데이터 셋을 학습하여 시간을 절약하고 있다. 이는 데이터 확장 법칙에 어느 정도 유연성이 있음을 시사한다. 이러한 데이터 셋의 존재는 <strong>‘데이터는 생각대로 동작하지 않는다’</strong>의 사고 방식에서 비롯된 것으로, Google 외부에서 학습을 수행하는 표준 방식으로 빠르게 자리 잡고 있다. 이러한 데이터 셋은 합성 방법(예: 기존 모델에서 최상의 응답을 필터링)과 다른 프로젝트에서 스크래빙을 통해 구축되며, 이 두 가지 방법 중 어느 것도 Google에서 널리 사용되는 방법은 아니지만 다행히도 이러한 고품질 데이터 셋은 오픈 소스이므로 무료로 사용할 수 있다.</p>

<p><br /></p>

<h2 id="directly-competing-with-open-source-is-a-losing-proposition">Directly Competing With Open Source Is a Losing Proposition</h2>

<p><strong>오픈소스와 직접 경쟁하는 것은 손해이다.</strong></p>

<p>이러한 최근의 진전은 Google의 비즈니스 전략에 직접적이고 즉각적인 영향을 미친다. <u>사용 제한이 없는 고품질의 무료 대안이 있는데 누가 사용 제한이 있는 Google 제품에 비용을 지불할 것인가?</u></p>

<p>그리고 우리가 따라잡을 수 있을 거라고 기대해서는 안된다. 현대 인터넷이 오픈소스로 운영되는 데에는 이유가 있으며, 오픈소스에는 우리가 복제할 수 없는 몇 가지 중요한 이점이 있다.</p>

<p><br /></p>

<h2 id="we-need-them-more-than-they-need-us">We need them more than they need us</h2>

<p><strong>고객이 우리를 필요로 하는 것보다 우리가 고객을 더 필요로 한다.</strong></p>

<p>기술을 비밀로 유지하는 것은 항상 어려운 일이었다. Google 연구원들은 정기적으로 다른 회사로 이직하기 때문에 우리가 알고 있는 모든 것을 알고 있고, 파이프라인이 열려 있는 한 계속 그럴 것이라고 생각할 수 있다.</p>

<p>하지만 LLM의 최첨단 연구 비용이 저렴해지면서 기술 경쟁 우위를 유지하는 것은 더욱 어려워졌다. 전 세계의 연구 기관들이 서로의 연구를 기반으로 솔루션 영역을 넓혀가고 있으며, 우리의 역량을 훨씬 능가하는 폭넓은 방식으로 솔루션을 탐색하고 있다. 우리는 외부의 혁신이 그 가치를 희석시키는 동안 우리의 비밀을 굳건히 지키려고 노력할 수도 있고, 서로에게서 배우려고 노력할 수도 있다.</p>

<p><br /></p>

<h2 id="individuals-are-not-constrained-by-licenses-to-the-same-degree-as-corporations">Individuals are not constrained by licenses to the same degree as corporations</h2>

<p><strong>개인은 기업과 같은 수준의 라이센스 제약을 받지 않는다.</strong></p>

<p>이러한 혁신의 대부분은 Meta에서 유출된 모델의 가중치를 기반으로 이루어지고 있다. <a href="https://bigscience.huggingface.co/blog/bloom">진정한 개방형 모델</a>이 개선됨에 따라 이러한 상황은 필연적으로 변화하겠지만, 중요한 것은 기다릴 필요가 없다는 것이다. <strong>‘개인적 사용’</strong>이 제공하는 법적 보호와 개인에 대한 기소의 비현실성 때문에 개인은 이러한 기술이 뜨거울 때 접근하고 있다.</p>

<p><br /></p>

<h2 id="being-your-own-customer-means-you-understand-the-use-case">Being your own customer means you understand the use case</h2>

<p><strong>고객이 된다는 것은 유스 케이스를 이해한다는 의미이다.</strong></p>

<p>이미지 생성 분야에서 사람들이 만드는 모델을 살펴보면 애니메이션 제너레이터부터 HDR 랜드스케이프에 이르기까지 방대한 창의성이 쏟아져 나오고 있다. 이러한 모델은 특정 하위 장르에 깊이 몰입한 사람들이 사용하고 만들었기 때문에 우리가 따라올 수 없는 깊이 있는 지식과 공감을 제공한다.</p>

<p><br /></p>

<h2 id="owning-the-ecosystem-letting-open-source-work-for-us">Owning the Ecosystem: Letting Open Source Work for Us</h2>

<p>역설적이게도 이 모든 것에서 확실한 승자는 <strong>Meta</strong>이다. 유출된 모델이 자신들의 것이었기 때문에, 그들은 사실상 <u>지구 전체에 해당하는 무료 노동력을 확보한 셈</u>이다. 대부분의 오픈소스 혁신이 Meta의 아키텍처를 기반으로 이루어지고 있기 때문에, Meta가 이를 자사 제품에 통합하는 것을 막을 수 있는 방법은 없다.</p>

<p>생태계를 소유하는 것의 가치는 아무리 강조해도 지나치지 않는다. Google은 Chrome 및 Android와 같은 오픈 소스 제품에서 이 패러다임을 성공적으로 활용하고 있다. 혁신이 일어나는 플랫폼을 소유함으로써 Google은 thought leader이자 direction-setter로서의 입지를 굳히고, 자신보다 더 큰 아이디어에 대한 내러티브를 형성할 수 있는 능력을 얻게 된다.</p>

<p><strong>모델을 더 엄격하게 제어할수록 개방형 대안 모델을 더 매력적으로 만들 수 있다.</strong> Google과 OpenAI는 모두 모델 사용 방식을 엄격하게 통제할 수 있는 릴리스 패턴에 방어적으로 끌려왔다. 하지만 이러한 통제는 허구이다. <u>승인되지 않은 목적으로 LLM을 사용하고자 하는 사람은 누구나 자유롭게 사용할 수 있는 모델 중 원하는 것을 선택하면 된다.</u></p>

<p>Google은 오픈 소스 커뮤니티의 리더로서 폭넓은 대화를 무시하지 말고 협력하여 주도권을 잡아야 한다. 이는 아마도 작은 ULM 변형에 대한 모델 가중치를 게시하는 것과 같은 불편한 조치를 취하는 것을 의미할 것이다. 이는 모델에 대한 일부 통제권을 포기하는 것을 의미한다. 하지만 이러한 타협은 불가피하다. 혁신을 주도하면서 동시에 혁신을 통제할 수는 없기 때문이다.</p>

<p><br /></p>

<h2 id="epilogue-what-about-openai">Epilogue: What about OpenAI?</h2>

<p>오픈소스에 대한 이 모든 이야기는 OpenAI의 현재 폐쇄적인 정책을 고려할 때 불공평하게 느껴질 수 있다. <u>저들은 공유하지 않을 텐데 우리는 왜 공유해야 할까?</u> 하지만 사실 우리는 이미 고급 연구원이 꾸준히 유출되는 형태로 모든 것을 그들과 공유하고 있다. 이러한 흐름을 막기 전까지는 비밀 유지에 대한 논의는 무의미하다.</p>

<p>결국 OpenAI는 중요하지 않다. 오픈소스와 관련하여 그들도 우리와 같은 실수를 저지르고 있으며, 우위를 유지할 수 있는 능력에 의문이 제기될 수밖에 없다. 오픈소스 대안이 그들의 입장을 바꾸지 않는 한 결국에는 오픈소스가 그들을 잠식할 수 있고 잠식할 것이다. 이 점에서 적어도 우리가 먼저 움직일 수 있다.</p>

<p><br /></p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">https://www.semianalysis.com/p/google-we-have-no-moat-and-neither</a></li>
</ul>

<p><br /></p>]]></content><author><name>Eunsoo Kang</name><email>me@eunsour.dev</email></author><category term="NLP" /><summary type="html"><![CDATA[아래 내용들은 Google 내부에서 유출된 문서로 그 진위 여부를 확인한 내용이며, 회사 전체가 아닌 Google 직원의 의견일 뿐이라고 말하고 있다.]]></summary></entry><entry><title type="html">[리뷰] LoRA: Low-Rank Adaptation of Large Language Models</title><link href="http://localhost:4000/LoRA/" rel="alternate" type="text/html" title="[리뷰] LoRA: Low-Rank Adaptation of Large Language Models" /><published>2023-11-26T11:58:47+09:00</published><updated>2023-11-26T11:58:47+09:00</updated><id>http://localhost:4000/LoRA</id><content type="html" xml:base="http://localhost:4000/LoRA/"><![CDATA[<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h2 id="introduce">Introduce</h2>

<p><code class="language-plaintext highlighter-rouge">PEFT(Parameter Efficient Fine-Tuning)</code>는 모델의 모든 파라미터를 튜닝하는 것이 아닌 일부 파라미터만을 튜닝함으로써 모델의 성능을 적은 자원으로도 높게 유지하는 방법론이다.</p>

<p>GPT-3와 같은 매우 큰 언어 모델이 등장하면서 다양한 문제들을 쉽게 해결할 수 있게 되었지만, 일반 사용자의 하드웨어로는 완전한 파인 튜닝이 불가능해졌다. 또한 각 다운스트림 태스크에 대해 파인 튜닝된 모델을 저장하고 배포하는 것은 많은 비용이 든다. 왜냐하면 파인 튜닝된 모델은 원래 사전 학습 모델과 크기가 동일하기 때문이다. <strong>PEFT</strong>는 두 가지 문제를 모두 해결하기 위한 방법을 제시한다.</p>

<p><strong>PEFT 접근 방식</strong>은 사전 학습된 LLM의 대부분의 파라미터를 동결하면서 소수의 모델 파라미터(ex. 0.01%)만 파인 튜닝하므로 계산 및 저장 비용이 크게 절감되며 이는 LLM의 파인 튜닝 중에 발생하는 <u>catastrophic forgetting</u>를 극복하기도 한다. (미리 학습된 Pretrained weights는 고정된 상태로 유지되기 때문에) <br />
또한 각 다운스트림 데이터 세트에 대해 발생하는 체크포인트는 몇 MB에 불과하면서도 full fine-tuning에 필적하는 성능을 달성할 수 있다. 학습된 작은 가중치는 사전 학습된 LLM에 추가되기 때문에 전체 모델을 교체할 필요 없이 작은 가중치를 추가하여 동일한 LLM을 여러 작업에 사용할 수 있다.</p>

<p>현재 PEFT를 위한 다양한 방법론들이 연구되고 있으며, 그 중 가장 유명한 것 중 하나가 <u>Stable diffusion</u>이나 <u>LLaMA</u>, <u>Alpaca</u>에도 많이 적용되는 Microsoft에서 공개한 <strong>LoRA</strong>이다.</p>

<p><br /></p>

<h2 id="lora">LoRA</h2>

<h3 id="introduction">Introduction</h3>

<p>전이 학습이 시작된 이래로 수십 개의 연구에서 model adaptation의 파라미터 및 컴퓨팅 효율성을 높이기 위해 노력해왔었다. 대표적인 방법은 <u>어댑터 레이어를 추가</u>하거나 <u>입력 레이어 activation을 optimizing</u>하는 것이다. 하지만 두 방법 모두 latency에 민감한 대규모 프로덕션에는 한계가 있었다.</p>

<center><img src="/assets/img/lora/0.png" style="zoom: 50%;" /></center>

<p>LoRA는 GPT-3와 같은 거대 모델들이 <u>over-parametrize</u> 되있으며, 실제 다운스트림 태스크를 해결하는 데 필요한 정보는 일부이며, 이 정보는 <strong>low intrinsic dimension</strong>에 있을거라는 연구에서 영감을 얻어서 시작되었다.</p>

<p>LoRA를 사용하면 위 그림과 같이 사전 학습된 가중치를 고정된 상태로 유지하면서 오른쪽의 dense layer만 학습하는데, dense layer의 weight를 low-rank로 decomposition한 matrices만을 optimizing하는 방식이다. 그래서 위 Figure 1과 같이 파인 튜닝 시에는 pre-trained weights \(W\)는 freeze하고, low-rank decomposition된 weights \(A\), \(B\)만 학습하여 W에 summation하게 된다.</p>

<p><br /></p>

<p><strong>LoRA를 사용하면 다음과 같은 이점이 있다.</strong></p>

<ul>
  <li>사전 학습된 모델을 공유하면서 다양한 태스크를 위한 여러 개의 작은 LoRA 모듈을 구축하는 데 사용할 수 있다. pre-trained weights(\(W_0\))을 고정하고 Figure 1의 행렬 A와 B만 A’, B’로 교체하면 되기 때문에 <u>스토리지 요구 사항</u>과 <u>task-switching 오버헤드</u>를 크게 줄일 수 있다.</li>
  <li>다운스트림 태스크마다 파인 튜닝을 모두 다시 해야하는 다른 모델들 대비 LoRA를 사용하면 low-rank matrices만 최적화하면 되기 때문에, <u>학습 효율을 높이고 하드웨어 진입 장벽을 최대 3배까지 낮출 수 있다.</u></li>
  <li>배포 시 훈련 가능한 행렬을 pre-trained weights(\(W_0\))와 병합할 수 있으므로, fully fine-tuned 모델에 비해 inference latency가 발생하지 않는다.</li>
  <li>GPT-3의 경우 12,888에 해당하는 원본(full-rank)정보를 r = 1, 2 정도의 매우 낮은 rank에서도 표현 가능하다는 것을 밝혔다.</li>
</ul>

<p><br /></p>

<h3 id="problem-statement">Problem Statement</h3>

<p>LoRA는 dense layer가 존재하는 모든 신경망에 적용이 가능하지만, 여기서는 language modeling에 초점을 맞추고 있다.<br /><br />
먼저 \(\Phi\) 로 parameterize 되어있는 pretrained autoregressive language model \(P\Phi(y|x)\) 이 주어졌다고 가정한다.
\(P\Phi(y|x)\) 는 GPT와 같은 일반적인 multi-task learner일 수 있다.<br /><br />
Adaptation을 위해 각 다운스트림 태스크는 컨텍스트 - 타켓 쌍의 훈련 데이터셋 집합 \(Z = {{(x_i, y_i)}}{i=1,..,N},\) 로 표현되며, 여기서 \(x_i\)와 \(y_i\)는 모두 토큰 시퀀스이다.</p>

<p><img src="/assets/img/lora/1.png" alt="" /></p>

<p>full fine-tuning의 경우 위처럼 모델은 pretrained weights \(Phi_0\) 으로 초기화되고 conditional language modeling objective를 최대화하기 위해 기울기를 반복적으로 \(\Phi_0 + \triangle\Phi\)로 업데이트한다.<br /><br />
full fine-tuning의 주요 단점 중 하나는 각 다운스트림 태스크마다 \(|\Phi_0|\)과 같은 차원의 크기를 가진 \(|\triangle\Phi|\)를 재학습해야 한다는 점이다.
따라서 사전 학습된 모델이 큰 경우(예: \(|\Phi_0|\) ≈ 175억인 GPT-3), 파인 튜닝된 모델의 많은 인스턴스를 저장하고 배포하는 데에 어려움이 있다.</p>

<p><img src="/assets/img/lora/2.png" alt="" /></p>

<p>LoRA는 업데이트 해야하는 task-specific parameter를 \(\triangle\Phi = \triangle\Phi(\Theta)\) 와 같이 더 작은 크기의 매개변수 \(\Theta\)로 인코딩하는 보다 효율적인 접근 방식을 채택한다 (\(\Theta \ll \Phi_0\)).<br />
따라서 \(|\triangle\Phi|\)를 찾는 작업은 \(\Theta\)에 대한 최적화로 대체된다.<br /><br />
위와 같은 방식으로 GPT-3를 fine-tuning할 경우, 학습해야 할 파라미터 \(|\Theta|\)의 수는 \(|\Phi_0|\)의 0.01%만큼 줄어들 수 있다.</p>

<p><br /></p>

<h3 id="our-method">Our Method</h3>

<h4 id="low-rank-parametrized-update-matrices">LOW-RANK-PARAMETRIZED UPDATE MATRICES</h4>

<p>신경망에는 행렬 곱셈을 수행하는 많은 dense layer가 있으며, 이러한 계층의 가중치 행렬은 일반적으로 full-rank를 갖는다. 하지만 LoRA는 Aghajanyan et al. (2020)에 착안하여 adaptation 과정에서 low intrinsic rank를 가진 가중치로 업데이트하는 방법이다.</p>

<p>먼저 pretrained weights matrix  \(W_0 ∈ R^{d\times k}\)에 대해 \(W_0 + ∆W = W_0 + BA\)로 업데이트한다. 즉, \(W_0\)는 동결하고 low-rank로 decomposition된 \(B ∈ R^{d\times r}\)와 \(A ∈ R^{r\times k}\)만을 학습한다. 그리고 \(W_0\)와 \(∆W = BA\) 모두 동일한 input에 곱해지며, 각각의 출력 벡터는 coordinate-wise로 합산된다.</p>

<p>forward pass 산출식은 다음과 같다.</p>

<p><img src="/assets/img/lora/3.png" alt="" /></p>

<p>A는 random Gaussian initialization, B는 0으로 초기화되므로 훈련이 시작될 때 \(∆W = BA\)는 0이 된다. 그런 다음 \(∆W x\)를 \(a \over r\)로 스케일링한다. Adam으로 최적화할 때 \(a\)를 조정하는 것은 학습 속도를 조정하는 것과 거의 동일하다고 한다. 따라서 \(a\)를 처음 \(r\) 값으로 설정하고 스케일링은 \(r\)을 변경할 때 하이퍼파라미터를 재조정할 필요성을 줄이는 데 도움이 된다.</p>

<p><br /></p>

<p><strong>A Generalization of Full Fine-tuning</strong></p>

<p>LoRA는 rank \(r\)을 pre-trained weights 행렬의 rank로 설정하여 full fine-tuning의 표현력을 회복하거나 조정할 수 있다. 따라서 r을 늘릴수록 LoRA 학습은 대략 원래 모델을 학습하는 것으로 수렴하는 반면, 어댑터 기반 방식은 MLP로, 접두사 기반 방식은 긴 입력 시퀀스를 사용할 수 없는 모델로 수렴한다. 
<br /><br /></p>

<p><strong>No Additional Inference Latency</strong></p>

<p>프로덕션에 배포할 때에는 \(W = W_0 + BA\)를 명시적으로 계산하여 저장하고 평소와 같이 추론을 수행할 수 있다. 만약 다른 다운스트림 태스크로 전환해야 하는 경우, \(BA\)를 뺀 다음 다른 \(B’A’\)을 추가하면 되기 때문에, 메모리 오버헤드가 거의 없으며, 이렇게 하면 태스크별로 파인 튜닝된 모델에 비해 추론하는 동안 추가 지연 시간이 발생하지 않는다.
<br /><br /></p>

<h4 id="applying-lora-to-transformer">APPLYING LORA TO TRANSFORMER</h4>

<p>LoRA는 dense layer가 존재하는 모든 신경망에 적용 가능하지만, 논문에서는 단순성과 파라미터 효율성을 위해 트랜스포머 아키텍처에서 attention weights만 조정하고 MLP 모듈을 동결하였다. (논문에서는 \(W_q\), \(W_v\)만 사용)
<br /><br /></p>

<p><strong>Practical Benefits and Limitations</strong></p>

<p>LoRA의 가장 큰 이점은 메모리 및 스토리지 사용량 감소에서 비롯된다. Adam으로 학습된 대형 트랜스포머의 경우, 고정된 파라미터에 대한 최적화 상태를 저장할 필요가 없으므로 VRAM 사용량을 최대 2/3까지 줄일 수 있다. GPT-3 175B에서는 훈련 중 VRAM 소비량을 1.2TB에서 350GB로 줄였다고 한다.</p>

<p>또한, \(r = 4\)이고 \(W_q\), \(W_v\) 행렬에만 LoRA를 적용할 때 체크포인트 크기가 약 10,000배(350GB에서 35MB로) 줄어들며, 대부분의 파라미터에 대한 기울기를 계산할 필요가 없기 때문에 GPT-3 175B에서 훈련하는 동안 전체 파인 튜닝에 비해 25%의 속도 향상을 관찰할 수 있었다.</p>

<p>하지만 LoRA에도 한계점이 있는데, LoRA를 적용할 weight matrices를 선택할 때 대부분 휴리스틱에 의존하기 때문에 조금 더 원칙적인 방법이 필요하다고 한다. 
<br /><br /></p>

<h3 id="empirical-experiments">Empirical Experiments</h3>

<center><img src="/assets/img/lora/4.png" style="zoom: 50%;" /></center>

<p>LoRA에 대한 최종 테스트로 GPT-3 175B 모델을 사용하였고, 위의 표에서 볼 수 있듯이 세 가지 데이터 셋 모두에서 파인 튜닝 모델의 베이스라인과 일치하거나 이를 초과하는 모습을 볼 수 있다.</p>

<p><br /></p>

<h3 id="using-peft-with-huggingface">Using PEFT with Huggingface</h3>

<p>LLaMA의 변형인 Alpaca에도 LoRA가 적용된 오픈소스 프로젝트들이 공개되고 있다.</p>

<p>HuggingFace의 PEFT 라이브러리를 이용하면 LoRA를 쉽게 적용해볼 수 있으며, 지원하는 PEFT 기법과 태스크들은 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PeftType</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PROMPT_TUNING</span> <span class="o">=</span> <span class="s">"PROMPT_TUNING"</span>
    <span class="n">MULTITASK_PROMPT_TUNING</span> <span class="o">=</span> <span class="s">"MULTITASK_PROMPT_TUNING"</span>
    <span class="n">P_TUNING</span> <span class="o">=</span> <span class="s">"P_TUNING"</span>
    <span class="n">PREFIX_TUNING</span> <span class="o">=</span> <span class="s">"PREFIX_TUNING"</span>
    <span class="n">LORA</span> <span class="o">=</span> <span class="s">"LORA"</span>
    <span class="n">ADALORA</span> <span class="o">=</span> <span class="s">"ADALORA"</span>
    <span class="n">ADAPTION_PROMPT</span> <span class="o">=</span> <span class="s">"ADAPTION_PROMPT"</span>
    <span class="n">IA3</span> <span class="o">=</span> <span class="s">"IA3"</span>
    <span class="n">LOHA</span> <span class="o">=</span> <span class="s">"LOHA"</span>
    <span class="n">LOKR</span> <span class="o">=</span> <span class="s">"LOKR"</span>

<span class="k">class</span> <span class="nc">TaskType</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">SEQ_CLS</span> <span class="o">=</span> <span class="s">"SEQ_CLS"</span>
    <span class="n">SEQ_2_SEQ_LM</span> <span class="o">=</span> <span class="s">"SEQ_2_SEQ_LM"</span>
    <span class="n">CAUSAL_LM</span> <span class="o">=</span> <span class="s">"CAUSAL_LM"</span>
    <span class="n">TOKEN_CLS</span> <span class="o">=</span> <span class="s">"TOKEN_CLS"</span>
    <span class="n">QUESTION_ANS</span> <span class="o">=</span> <span class="s">"QUESTION_ANS"</span>
    <span class="n">FEATURE_EXTRACTION</span> <span class="o">=</span> <span class="s">"FEATURE_EXTRACTION"</span>
</code></pre></div></div>
<p><br />
LoRA의 적용은 다음과 같이 매우 간단하게 할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>                       <span class="c1"># LoRA의 dimension (rank 수) : 논문에서는 4를 사용
</span>    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>              <span class="c1"># learning_rate와 유사함. feed-forward 할 때 얼마나 반영할 지 
</span>    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>           <span class="c1"># LoRA 레이어의 드롭아웃 확률 
</span>    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"q"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">],</span>    <span class="c1"># LoRA를 적용할 self-attention 모듈
</span>    <span class="n">bias</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>               <span class="c1"># 훈련 중 bias 업데이트 할 지 유무 ["none", "all", "lora_only"]
</span>    <span class="n">task_type</span><span class="o">=</span><span class="s">"SEQ_2_SEQ_LM"</span><span class="p">,</span>   <span class="c1"># 본인의 태스크에 맞는 타입을 명시하면 됨
</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<p>LLaMA와 같은 foundation 모델과 더불어 LoRA와 같은 PEFT 기법을 활용해서 거대 언어 모델을 개인이 쉽게 파인 튜닝해보고 사용해볼 수 있는 시대가 빠르게 다가오고 있는 것 같다.</p>

<p><br /></p>]]></content><author><name>Eunsoo Kang</name><email>me@eunsour.dev</email></author><category term="NLP" /><category term="LLM" /><category term="PEFT" /><summary type="html"><![CDATA[]]></summary></entry></feed>