<!DOCTYPE html> <html lang="ko-KR"> <head prefix="og: http://ogp.me/ns#"> <meta charset="UTF-8" /> <meta http-equiv="X-UA-Compatible" content="ie=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1.0" /> <meta name="mobile-web-app-capable" content="yes" /> <meta name="apple-mobile-web-app-capable" content="yes" /> <meta name="application-name" content="Eunsoo Kang" /> <meta name="apple-mobile-web-app-status-bar-style" content="#fff" /> <meta name="apple-mobile-web-app-title" content="Eunsoo Kang" /> <title> [리뷰] LLaMA: Open and Efficient Foundation Language Models - Eunsoo Kang </title> <link rel="alternate" href="http://localhost:4000/LLaMA-1/" hreflang="ko-KR" /> <link rel="canonical" href="http://localhost:4000/LLaMA-1/" /> <meta name="description" content="Joy of Growth" /> <meta name="referrer" content="no-referrer-when-downgrade" /> <meta property="fb:app_id" content="" /> <meta property="og:site_name" content="[리뷰] LLaMA: Open and Efficient Foundation Language Models | Eunsoo Kang" /> <meta property="og:title" content="[리뷰] LLaMA: Open and Efficient Foundation Language Models | Eunsoo Kang" /> <meta property="og:type" content="website" /> <meta property="og:url" content="http://localhost:4000/LLaMA-1/" /> <meta property="og:description" content="Joy of Growth" /> <meta property="og:image" content="http://localhost:4000/assets/img/ogp.png" /> <meta property="og:image:width" content="640" /> <meta property="og:image:height" content="640" /> <meta name="twitter:card" content="summary" /> <meta name="twitter:title" content="[리뷰] LLaMA: Open and Efficient Foundation Language Models | " /> <meta name="twitter:url" content="http://localhost:4000/LLaMA-1/" /> <meta name="twitter:site" content="@" /> <meta name="twitter:creator" content="@" /> <meta name="twitter:description" content="Joy of Growth" /> <meta name="twitter:image" content="http://localhost:4000/assets/img/ogp.png" /> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Eunsoo Kang" /> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/favicon.svg" /> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon.svg" /> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon.svg" /> <link rel="manifest" href="/assets/favicons/site.webmanifest" /> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#5bbad5" /> <meta name="apple-mobile-web-app-title" content="Jekyll Klise" /> <meta name="application-name" content="Jekyll Klise" /> <meta name="msapplication-TileColor" content="#da532c" /> <meta name="theme-color" content="#2c2c2c" /> <link rel="stylesheet" href="/assets/css/style.css" /> <!-- for mathjax support --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body data-theme="light" class="notransition"> <script> const body = document.body; const data = body.getAttribute("data-theme"); const initTheme = (state) => { if (state === "dark") { body.setAttribute("data-theme", "dark"); } else if (state === "light") { body.removeAttribute("data-theme"); } else { localStorage.setItem("theme", data); } }; initTheme(localStorage.getItem("theme")); setTimeout(() => body.classList.remove("notransition"), 75); </script> <div class="navbar" role="navigation"> <nav class="menu"> <input type="checkbox" id="menu-trigger" class="menu-trigger" /> <label for="menu-trigger"> <span class="menu-icon"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <path d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z" /> </svg> </span> </label> <a id="mode"> <svg class="mode-sunny" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>LIGHT</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> <svg class="mode-moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>DARK</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> </a> <div class="trigger"> <div class="trigger-container"><a class="menu-link" href="/">Home</a><a class="menu-link" href="/archive/">Archive</a><a class="menu-link" href="/about/">About</a><a class="menu-link rss" href="/feed.xml"> <svg xmlns="http://www.w3.org/2000/svg" width="17" height="17" viewBox="0 0 512 512" fill="#ED812E" > <title>RSS</title> <path d="M108.56,342.78a60.34,60.34,0,1,0,60.56,60.44A60.63,60.63,0,0,0,108.56,342.78Z" /> <path d="M48,186.67v86.55c52,0,101.94,15.39,138.67,52.11s52,86.56,52,138.67h86.66C325.33,312.44,199.67,186.67,48,186.67Z" /> <path d="M48,48v86.56c185.25,0,329.22,144.08,329.22,329.44H464C464,234.66,277.67,48,48,48Z" /> </svg> </a> </div> </div> </nav> </div> <div class="wrapper post"> <main class="page-content" aria-label="Content"> <article itemscope itemtype="https://schema.org/BlogPosting"> <header class="header"> <div class="tags"> <span itemprop="keywords"> <a class="tag" href="/tags/#nlp">NLP</a>, <a class="tag" href="/tags/#llm">LLM</a>, <a class="tag" href="/tags/#generative-ai">GENERATIVE AI</a>, <a class="tag" href="/tags/#foundation-models">FOUNDATION MODELS</a> </span> </div> <h1 class="header-title" itemprop="headline">[리뷰] LLaMA: Open and Efficient Foundation Language Models</h1> <div class="post-meta"> <time datetime="2023-12-24T11:58:47+09:00" itemprop="datePublished"> Dec 24, 2023 </time> <span itemprop="author" itemscope itemtype="https://schema.org/Person"> <span itemprop="name">Eunsoo Kang</span> </span> <time hidden datetime="2023-12-24T11:58:47+09:00" itemprop="dateModified"> Dec 24, 2023 </time> <span hidden itemprop="publisher" itemtype="Person">Eunsoo Kang</span> <span hidden itemprop="image"></span> <span hidden itemprop="mainEntityOfPage"><h2 id="introduction">Introduction</h2> </span> </div> </header> <div class="page-content" itemprop="articleBody"> <h2 id="introduction"> <a href="#introduction" class="anchor-head"></a> Introduction </h2> <p>최근 몇 년간, 언어 모델의 크기는 <a href="https://arxiv.org/abs/2001.08361">Scaling laws</a>에 따라 지속적으로 증가하는 추세였다. 이는 모델의 성능 향상에는 기여했지만, 모델을 서빙할 때의 추론 예산을 무시하는 문제점이 있었다. 따라서 기업들은 목표 성능을 달성한 후에는 학습 속도가 아닌 <u>추론 속도가 가장 빠른 모델</u>을 선호하는 경향이 있다.</p> <p>2022년 5월 Meta에서 <a href="https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/">OPT-175B</a>를 발표하였다. 하지만 OPT는 <a href="https://github.com/openai/gpt-3">GPT-3</a>와 동일한 모델 크기를 갖고 있음에도 불구하고 성능에서 큰 차이가 있었다. 논문에서도 모델의 성능을 자랑하는 것보다는 왜 이 모델이 성능이 부족했는지에 대한 탐구에 중점을 두고 있다.</p> <p>이후 Meta는 절치부심해서 새로운 모델을 개발하였으며, Google Deepmind에서 발표한 <a href="https://arxiv.org/abs/2203.15556">Chinchilla Scaling Laws</a> 연구 결과에 영감을 받아 GPT-3(175B)보다 더 작으면서 고성능인 <strong>LLaMA</strong>을 선보였다.</p> <p>GPT-3가 1.3B부터 175B까지 네 가지 모델을 제공한 것과 마찬가지로, LLaMA도 7B, 13B, 33B, 65B의 네 가지 모델을 제공한다.</p> <p><br /></p> <h2 id="approach"> <a href="#approach" class="anchor-head"></a> Approach </h2> <h3 id="pre-training-data"> <a href="#pre-training-data" class="anchor-head"></a> Pre-training Data </h3> <p><strong>LLaMA</strong>는 <u>공개적으로 사용 가능</u>하고 <u>오픈 소스와 호환되는 데이터</u>만 사용한다는 제한을 두고 다른 LLM을 학습하는 데 활용된 데이터 셋을 재사용하였다. 사용된 데이터셋과 데이터의 비율은 아래와 같다.</p> <center><img src="/assets/img/llama_1/0.png" style="zoom: 65%;" /></center> <p>데이터셋 구성을 보면 <strong>CommonCrawl</strong>, <strong>C4</strong>의 합계가 82%로, 이는 GPT-3에 투입된 비율과 같다.</p> <p>웹 크롤링 데이터는 품질이 낮기 때문에 정제를 잘하는 것이 중요하다. 이를 위해 Meta에서는 별도의 모델을 만들어 수집한 페이지가 Wikipedia의 인용 출처로 쓰일 수 있는지 여부를 판별하는 <u>분류기를 학습</u>시키고, 이를 통과한 페이지만 학습에 투입하였다.</p> <p>다음으로 Google BigQuery에서 사용되는 공개 <strong>GitHub</strong> 데이터셋을 사용하였다. 코드 데이터의 논리성이 언어 학습에 도움을 준다고 한다.</p> <p>이외에 2022년 6월부터 8월까지 20개국의 <strong>Wikipedia</strong> 데이터가 높은 비중으로 학습에 투입되었고, 논리적인 태스크 수행을 위해 <strong>도서</strong>와 <strong>논문</strong> 데이터도 포함된 것으로 보인다.</p> <p>토크나이저는 OPT에서 사용했던 GPT 스타일의 <a href="https://arxiv.org/abs/1508.07909">BBPE</a>에서 <a href="https://github.com/google/sentencepiece">BPE</a>로 변경하였고, Unknown 토큰을 대비하여 폴백 옵션을 추가하였다.</p> <p><br /></p> <h3 id="architecture"> <a href="#architecture" class="anchor-head"></a> Architecture </h3> <p>LLaMA의 아키텍처는 <u>Vanilla Transformer 아키텍처</u>에서 빠른 학습을 위해 일부 구조를 개선하였다.</p> <ul> <li><strong>Pre-normalization</strong> [from GPT-3] <ul> <li>Vanilla Transformer와 달리 attention 전에 normalization을 먼저 수행하는 Pre-normalization을 적용하였으며, <a href="https://github.com/google-research/t5x">T5</a>와 마찬가지로 <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>을 사용하였다.</li> </ul> </li> <li><strong>SwiGLU activation function</strong> [from <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM</a>] <ul> <li>성능 개선을 위해 ReLU를 SwiGLU로 교체하였으며, PaLM과 달리 \(4d\) 가 아닌 \(\frac{2}{3}4d\) 를 사용하였다.</li> <li><a href="https://arxiv.org/abs/1710.05941v1">Swish</a>와 <a href="https://arxiv.org/abs/1612.08083">GLU</a>를 합친 것인데 ReLU보다 부드러운 곡선을 그리며, <a href="https://arxiv.org/abs/1606.08415">GELU</a>와 비슷하지만 연산이 간단하다는 장점이 있다.</li> </ul> </li> <li><strong>Rotary Embeddings</strong> [from <a href="https://github.com/EleutherAI/gpt-neo">GPTNeo</a>] <ul> <li>absolute positional embeddings를 제거하는 대신 <a href="https://arxiv.org/abs/2104.09864">RoPE (rotary positional embeddings)</a> 방식을 사용하여 학습 시 2048 토큰보다 긴 입력도 처리할 수 있다.</li> </ul> </li> </ul> <p><br /></p> <h3 id="optimizer"> <a href="#optimizer" class="anchor-head"></a> Optimizer </h3> <p>LLaMA는 다음과 같은 하이퍼파라미터를 사용하여 <a href="https://arxiv.org/pdf/1711.05101.pdf">AdamW Optimizer</a>를 통해 학습된다.</p> <ul> <li>\(\beta_1\) = 0.9, \(\beta_2\) = 0.95</li> <li>weight decay = 0.1</li> <li>gradient clipping = 1.0</li> <li>warmup steps = 2000</li> <li>최종 학습률이 최대 학습률의 10%가 되도록 cosine learning rate scheduler 사용</li> </ul> <p><br /></p> <center><img src="/assets/img/llama_1/1.png" /></center> <p><br /></p> <h3 id="efficient-implementation"> <a href="#efficient-implementation" class="anchor-head"></a> Efficient implementation </h3> <p>모델의 학습 속도 향상을 위해 몇 가지 최적화를 수행하였다.</p> <ol> <li>메모리 사용량과 런타임을 줄이기 위해 multi-head attention을 효율적으로 구현한 <code class="language-plaintext highlighter-rouge">xformers</code> 라이브러리를 사용하였다.</li> <li>backward pass시 linear layer의 출력과 같이 계산 비용이 많이 드는 activation을 그대로 저장함으로써 재계산되는 activation을 줄였다.</li> <li>activation의 계산과 네트워크를 통한 GPU 간의 통신을 최대한 중첩시켜 수행하였다.</li> </ol> <p><br /></p> <p>이로써 학습 시간은 LLaMA-65B 학습을 80G A100 2048장, 256노드로 진행해서 1.4T 토큰 학습에 21일 걸렸다고 한다.</p> <center><img src="/assets/img/llama_1/2.png" style="zoom: 65%;" /></center> <p><br /></p> <h2 id="main-results"> <a href="#main-results" class="anchor-head"></a> Main results </h2> <p>저자들은 Zero-shot 및 Few-shot task를 포함한 다양한 벤치마크에서 GPT-3, Gopher, Chinchilla, PaLM과 같은 Foundation Model과 LLaMA를 비교하였다. 이중 대표적인 벤치마크에 대한 결과는 다음과 같다.</p> <center><img src="/assets/img/llama_1/3.png" /></center> <ul> <li>LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B보다 성능이 뛰어나다.</li> <li>LLaMA-65B는 BoolQ와 WinoGrande를 제외한 모든 벤치마크에서 PaLM540B보다 성능이 뛰어나다.</li> <li>LLaMA-13B 또한 GPT-3에 비해 10배 더 작은 크기임에도 불구하고 PIQA, OBQA를 제외한 대부분의 벤치마크에서 우수한 성능을 보인다.</li> </ul> <p>해당 결과를 통해, 같은 모델 크기에서 LLaMA가 경쟁 모델 대비 우수한 성능을 보이며, 작은 모델에서도 일정 수준 이상의 성능을 발휘할 수 있음을 확인할 수 있다.</p> <p><br /></p> <h3 id="massive-multitask-language-understanding5"> <a href="#massive-multitask-language-understanding5" class="anchor-head"></a> Massive Multitask Language Understanding5 </h3> <center><img src="/assets/img/llama_1/4.png" /></center> <ul> <li><strong>MMLU</strong>는 모델의 멀티태스크 정확도를 측정하는 데이터셋으로 인문학, 초등 수학, 역사, 컴퓨터 과학, 법률 등 57개의 주제를 포함하고 있다. MMLU를 잘 수행하기 위해서는 모델이 광범위한 상식을 갖추고 다양한 문제를 해결할 수 있는 능력이 필요하다.</li> <li>LLaMA-65B는 GPT-3보다 더 우수하지만 Chinchilla와 PaLM에 비해 떨어지는 벤치마크 결과를 보인다.</li> <li>저자들은 책과 논문 데이터가 Gopher와 Chinchilla에 비해 적은 양으로 학습되었기 때문이라고 분석하고 있다. (LLaMA : 177GB, Gopher, Chinchilla : 2TB)</li> </ul> <p><br /></p> <h2 id="instruction-tuning"> <a href="#instruction-tuning" class="anchor-head"></a> Instruction Tuning </h2> <p>다음은 LLaMA-65B에 instruction tuning을 적용하고 MMLU에서 5-shot 평균을 낸 결과이다.</p> <p>아주 적은 양의 명령어 데이터 파인 튜닝만으로 MMLU의 성능이 향상되고 instruction 수행 능력이 더욱 향상되는 것을 관찰할 수 있었다. (63.4 → 68.9)</p> <p>하지만 77.4%라는 SOTA 성능을 끌어낸 OpenAI의 <a href="https://platform.openai.com/docs/models/gpt-3-5">code-davinci-002</a> 모델에는 미치지 못하는 모습을 볼 수 있다.</p> <center><img src="/assets/img/llama_1/5.png" style="zoom: 65%;" /></center> <p><br /></p> <h2 id="couclusion"> <a href="#couclusion" class="anchor-head"></a> Couclusion </h2> <p><strong>LLaMA-13B</strong>는 GPT-3의 10분의 1 크기에도 불구하고, 대부분의 벤치마크에서 GPT-3보다 높은 성능을 보였다. 한 발 더 나아가, LLaMA-65B는 Chinchilla, Gopher, GPT-3, PaLM 등의 경쟁 모델들과 비슷하거나 더 나은 결과를 보였다.</p> <p>OPT의 성능 저하 원인 중 하나는 180B 토큰으로 학습되어 학습량이 부족하다는 점이었다. (GPT-3의 경우 300B 토큰으로 학습) 이에 비해 1.4T 토큰으로 학습한 LLaMA는 더 많은 학습 데이터를 기반으로 뛰어난 성능을 보이는 것은 이전에 Chinchilla에서 확인된 바와 같이 예상 가능한 결과이다.</p> <p>더불어, Chinchilla보다 LLaMA의 성능이 대체로 높은 것으로 나타났는데, 이는 정제된 웹 크롤링 데이터를 활용한 것이 효과를 발휘한 것으로 해석할 수 있다.</p> <p>이후 LLaMA 연구진들은 향후 더 큰 모델에 더 많은 token을 학습시킨 모델을 발표할 계획이라고 밝혔다.</p> <p>그리고 2023년 7월, Meta AI팀은 학술적 및 상업적 이용이 전부 가능한 오픈소스 LLM으로 <a href="https://arxiv.org/abs/2307.09288">LLaMA-2</a>를 발표하였다. 이 모델은 LLaMA-1보다 40% 더 많은 데이터(2T)를 활용하고, context length도 2배로 늘어난(2048에서 4096으로) 특징을 가지고 있다.</p> <p><br /></p> <h3 id="reference"> <a href="#reference" class="anchor-head"></a> Reference </h3> <ul> <li><a href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a></li> <li><a href="https://devocean.sk.com/blog/techBoardDetail.do?ID=164601">https://devocean.sk.com/blog/techBoardDetail.do?ID=164601</a></li> </ul> <p><br /></p> </div> </article> </main> <small class="post-updated-at">updated_at 24-12-2023</small> <nav class="post-nav"> <a class="post-nav-item post-nav-prev" href="/google-we-have-no-moat-and-neither/" > <div class="nav-arrow">Previous</div> <span class="post-title">Google "We Have No Moat, And Neither Does OpenAI"</span> </a> <a class="post-nav-item post-nav-next" href="/chinchilla-scaling-laws/"> <div class="nav-arrow">Next</div> <span class="post-title">[리뷰] Training Compute-Optimal Large Language Models</span> </a> </nav> <footer class="footer"> <a class="footer_item" href="javascript::void(0)">resume</a> <a class="footer_item" href="/feed.xml">rss</a> <span class="footer_item">&copy; 2024</span> <small class="footer_copyright"> <!-- Klisé Theme: https://github.com/piharpi/jekyll-klise --> <a href="https://github.com/piharpi/jekyll-klise" target="_blank" rel="noreferrer noopener" >klisé</a > theme on <a href="https://jekyllrb.com" target="_blank" rel="noreferrer noopener" >jekyll</a > </small> </footer> <script src="/assets/js/main.js" defer="defer"></script> </div> </body> </html>
